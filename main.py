import tkinter as tk
from tkinter import ttk, messagebox
from tkinter import simpledialog 
import cv2
from PIL import Image, ImageTk
import threading
import time
import numpy as np

# [ì¤‘ìš”] ë°˜ë“œì‹œ ë‹¤ë¥¸ matplotlib importë³´ë‹¤ ìœ„ì— ìˆì–´ì•¼ í•¨
import matplotlib
matplotlib.use('TkAgg') 
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
import random
import os
import json
import re
import pyaudio
import wave
import audioop
import sys
import queue
import contextlib 
import difflib 
import math 

# [í•„ìˆ˜] MediaPipe
import mediapipe as mp

# [í•„ìˆ˜] Whisper (ê³ ì„±ëŠ¥ ë¶„ì„)
from faster_whisper import WhisperModel

# [í•„ìˆ˜] Vosk (ì‹¤ì‹œê°„)
from vosk import Model, KaldiRecognizer

def resource_path(relative_path):
    try:
        # PyInstallerê°€ ìƒì„±í•œ ì„ì‹œ í´ë” ê²½ë¡œ (.exe ì‹¤í–‰ ì‹œ)
        base_path = sys._MEIPASS
    except Exception:
        # í‰ì†Œ ê°œë°œ í™˜ê²½ (.py ì‹¤í–‰ ì‹œ)
        base_path = os.path.dirname(os.path.abspath(__file__))
    return os.path.join(base_path, relative_path)

try:
    import app_config 
    from question_generator import DynamicQuestionGenerator, IMRADValidator
    from analysis_manager import AnalysisManager
    from ai_rewriter import AI_Announcer 
except ImportError as e:
    print(f"ê²½ê³ : í•„ìš”í•œ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    class DynamicQuestionGenerator: 
        def __init__(self, *args): pass 
    class IMRADValidator: 
        def __init__(self, *args): pass 
    class AnalysisManager: 
        def __init__(self, *args): pass
    class AI_Announcer: 
        def __init__(self, *args): pass

# --- ì „ì—­ ë³€ìˆ˜ ì„¤ì • ---
is_recording = False
start_time = 0
speech_data = {"full_transcript": "", "word_count": 0, "filler_count": 0}
gaze_data = {"total_frames": 0, "looking_frames": 0, "script_frames": 0} # script_frames ì¶”ê°€
audio_data = {"volumes": [], "tremble_count": 0}
timeline_markers = []
cap = None
out = None
pa = pyaudio.PyAudio()

# MediaPipe ì´ˆê¸°í™”
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(
    max_num_faces=1,
    refine_landmarks=True, # ëˆˆë™ì(Iris) ì¶”ì ì„ ìœ„í•´ í•„ìˆ˜
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

# ì–¼êµ´ ì¸ì‹ ìµœì í™” ë³€ìˆ˜
current_face_box = None
frame_count = 0

# --- mainFinal.py ìˆ˜ì •í•  ë¶€ë¶„ ---

vosk_model = None
try:
    # 1ìˆœìœ„: ê°€ì¥ ë‹¨ìˆœí•œ ë°©ë²• (ì˜ ë˜ëŠ” ì½”ë“œì˜ ë°©ì‹)
    if os.path.exists("model"):
        vosk_model = Model("model")
        print("âœ… Vosk ì˜¤í”„ë¼ì¸ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (ìƒëŒ€ ê²½ë¡œ)")
        
    # 2ìˆœìœ„: ë§Œì•½ ìœ„ ë°©ë²•ì´ ì•ˆ ë˜ë©´ resource_path ì‚¬ìš© (PyInstaller ë“± ëŒ€ë¹„)
    else:
        model_path = resource_path("model")
        if os.path.exists(model_path):
            vosk_model = Model(model_path)
            print(f"âœ… Vosk ì˜¤í”„ë¼ì¸ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (ì ˆëŒ€ ê²½ë¡œ): {model_path}")
        else:
            print(f"âš ï¸ ê²½ê³ : ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

except Exception as e:
    print(f"âŒ Vosk ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")

class App(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title("AI Presentation Pro (Final Ver: Enhanced UI & Gaze)")
        self.geometry("1400x950") # í™”ë©´ì„ ì¢€ ë” ë„“ê²Œ ì„¤ì •
        
        # ê¸´ì¥ ëª¨ë“œ ìƒíƒœ ë³€ìˆ˜
        self.is_anxious = False
        self.heart_phase = 0.0
        
        if 'app_config' in globals() and hasattr(app_config, 'set_korean_font'):
            app_config.set_korean_font() 
        
        self.user_settings = {}
        self.original_script = ""
        self.style = ttk.Style()
        self.style.theme_use('clam')
        
        self.load_and_initialize_apis() 
        
        if 'app_config' in globals() and hasattr(app_config, 'STOPWORDS'):
            self.analysis_manager = AnalysisManager(app_config.STOPWORDS, app_config.COACHING_CONFIG)
            self.dynamic_generator = DynamicQuestionGenerator(self.text_model) 
            self.imrad_validator = IMRADValidator(self.text_model)
            self.ai_announcer = AI_Announcer(self.text_model) 
        else:
            self.analysis_manager = AnalysisManager({}, {})
            self.dynamic_generator = DynamicQuestionGenerator(None)
            self.imrad_validator = IMRADValidator(None)
            self.ai_announcer = AI_Announcer(None)

        self.extracted_keywords = []
        self.raw_audio_frames = []

        self.protocol("WM_DELETE_WINDOW", self.on_closing)
        self.load_history()
        self.show_setup_page()

    def load_and_initialize_apis(self):
        if 'app_config' not in globals() or not hasattr(app_config, 'load_api_keys'):
            self.AI_AVAILABLE = False
            self.text_model = None
            return

        gemini_key = app_config.load_api_keys()
        
        if not gemini_key:
            gemini_key = simpledialog.askstring("Gemini API í‚¤ í•„ìš”", 
                                                "Gemini API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš” (AI í”¼ë“œë°±ìš©):\n", 
                                                parent=self)
            if gemini_key:
                app_config.save_api_keys(gemini_key)

        self.text_model = None
        self.AI_AVAILABLE = False

        if gemini_key:
            try:
                if 'app_config' in globals() and hasattr(app_config, 'genai'):
                    app_config.genai.configure(api_key=gemini_key)
                    self.text_model = app_config.genai.GenerativeModel('gemini-2.5-pro')
                    self.AI_AVAILABLE = True
                    print("Gemini API ì—°ê²° ì„±ê³µ")
            except Exception as e:
                print(f"Gemini ì—°ê²° ì‹¤íŒ¨: {e}")
        else:
            print("Gemini API í‚¤ ì—†ìŒ.")

    def on_closing(self):
        global is_recording, cap, out, pa
        is_recording = False
        self.is_anxious = False 
        if cap and cap.isOpened(): cap.release()
        if out: out.release()
        if pa: pa.terminate() 
        try:
            for f in ["rewritten_script_output.wav", "output.avi", "output.wav"]:
                if os.path.exists(f): os.remove(f)
        except: pass
        self.destroy()
        os._exit(0)

    def load_history(self):
        self.history = []
        if 'app_config' in globals() and hasattr(app_config, 'HISTORY_FILE') and os.path.exists(app_config.HISTORY_FILE):
            try:
                with open(app_config.HISTORY_FILE, "r", encoding='utf-8') as f:
                    self.history = json.load(f)
            except: self.history = []

    def save_history(self, score):
        if 'app_config' not in globals() or not hasattr(app_config, 'HISTORY_FILE'): return
        self.history.append(score)
        with open(app_config.HISTORY_FILE, "w", encoding='utf-8') as f:
            json.dump(self.history, f, ensure_ascii=False, indent=4)

    def clear_window(self):
        self.unbind_all("<MouseWheel>")
        for widget in self.winfo_children(): widget.destroy()

    def show_setup_page(self):
        self.clear_window()
        frame = ttk.Frame(self)
        frame.pack(expand=True)
        ttk.Label(frame, text="ğŸ¤ AI Presentation Pro", font=("Arial", 30, "bold")).pack(pady=30)
        
        ttk.Label(frame, text="ë°œí‘œ ìœ í˜• ì„ íƒ:", font=("Arial", 14)).pack()
        self.atmosphere_var = tk.StringVar(value="ğŸ“˜ ì •ë³´ ì „ë‹¬í˜• (ì •í™•ì„± ì¤‘ì‹œ)")
        modes = ["ğŸ“˜ ì •ë³´ ì „ë‹¬í˜• (ì •í™•ì„± ì¤‘ì‹œ)", "ğŸ”¥ ì„¤ë“/ë™ê¸°ë¶€ì—¬í˜• (ì—ë„ˆì§€ ì¤‘ì‹œ)", "ğŸ¤ ê³µê°/ì†Œí†µí˜• (ë°¸ëŸ°ìŠ¤ ì¤‘ì‹œ)"]
        ttk.Combobox(frame, textvariable=self.atmosphere_var, values=modes, state="readonly", font=("Arial", 12), width=35).pack(pady=15)
        
        ttk.Button(frame, text="ì—°ìŠµ ì‹œì‘í•˜ê¸°", command=self.go_to_practice).pack(pady=20, ipadx=20, ipady=10)
        ttk.Button(frame, text="ğŸ“¢ AI ëŒ€ë³¸ ì¬ì‘ì„± (Gemini)", command=self.show_rewriter_window).pack(pady=10, ipadx=10, ipady=5)

    def go_to_practice(self):
        self.user_settings['atmosphere'] = self.atmosphere_var.get()
        self.show_practice_page()

    # =========================================================================
    # [UI ëŒ€ê·œëª¨ ìˆ˜ì •] í™”ë©´ ìƒë‹¨: ì²­ì¤‘/ë‚´ì–¼êµ´ ë³‘ë ¬ ë°°ì¹˜, í•˜ë‹¨: ëŒ€ë³¸ ìŠ¤í¬ë¡¤
    # =========================================================================
    def show_practice_page(self):
        self.clear_window()
        
        # ì „ì²´ ë©”ì¸ í”„ë ˆì„
        main_frame = ttk.Frame(self)
        main_frame.pack(fill='both', expand=True, padx=20, pady=20)

        # --- ìƒë‹¨ ì˜ì—­: í™”ë©´ ë¶„í•  (ì²­ì¤‘ | ë‚´ ì–¼êµ´) ---
        top_frame = ttk.Frame(main_frame)
        top_frame.pack(side='top', fill='both', expand=True, pady=(0, 10))
        
        # ìƒë‹¨ ê·¸ë¦¬ë“œ ì„¤ì • (1í–‰ 2ì—´, ê· ë“± ë¹„ìœ¨)
        top_frame.columnconfigure(0, weight=1) # ì²­ì¤‘ ì˜ì—­
        top_frame.columnconfigure(1, weight=1) # ë‚´ ì–¼êµ´ ì˜ì—­
        top_frame.rowconfigure(0, weight=1)

        # 1. ì²­ì¤‘ íŒ¨ë„ (ì™¼ìª½)
        self.audience_frame = tk.Frame(top_frame, bg="#e9ecef", bd=2, relief="sunken")
        self.audience_frame.grid(row=0, column=0, sticky="nsew", padx=(0, 10))
        
        # ì²­ì¤‘ ì´ë¯¸ì§€ê°€ ì¤‘ì•™ì— ì˜¤ë„ë¡ ë‚´ë¶€ í”„ë ˆì„ ì‚¬ìš©
        aud_inner = tk.Frame(self.audience_frame, bg="#e9ecef")
        aud_inner.pack(expand=True)
        self.aud_labels = [ttk.Label(aud_inner) for _ in range(2)]
        for lbl in self.aud_labels: lbl.pack(side="left", padx=5)

        # 2. ë‚´ ì–¼êµ´ íŒ¨ë„ (ì˜¤ë¥¸ìª½)
        video_bg_frame = tk.Frame(top_frame, bg="black", bd=2, relief="sunken")
        video_bg_frame.grid(row=0, column=1, sticky="nsew")
        
        self.video_panel = ttk.Label(video_bg_frame)
        self.video_panel.pack(expand=True)

        # ì´ˆê¸° ì²­ì¤‘ ì´ë¯¸ì§€ ì„¤ì •
        self.update_audience_images('default', 'default') 
        
        # --- ì¤‘ë‹¨ ì˜ì—­: ì»¨íŠ¸ë¡¤ ë²„íŠ¼ ---
        control_frame = ttk.Frame(main_frame)
        control_frame.pack(side='top', fill='x', pady=10)
        
        # ë²„íŠ¼ë“¤ ì¤‘ì•™ ì •ë ¬ì„ ìœ„í•œ ë‚´ë¶€ í”„ë ˆì„
        btn_box = ttk.Frame(control_frame)
        btn_box.pack(anchor='center')

        self.btn_start = ttk.Button(btn_box, text="â–¶ ë…¹í™” ì‹œì‘", command=self.start_recording)
        self.btn_start.pack(side="left", padx=10)
        
        self.btn_panic = tk.Button(btn_box, text="ğŸ˜° ê¸´ì¥ ëª¨ë“œ: OFF", font=("Arial", 10), bg="#dddddd", command=self.toggle_anxiety)
        self.btn_panic.pack(side="left", padx=10)
        
        self.btn_question = ttk.Button(btn_box, text="âš¡ï¸ ëŒë°œ ì§ˆë¬¸", command=self.trigger_question_event, state="disabled")
        self.btn_question.pack(side="left", padx=10)
        
        self.btn_stop = ttk.Button(btn_box, text="â–  ê²°ê³¼ ë³´ê¸°", command=self.stop_recording, state="disabled")
        self.btn_stop.pack(side="left", padx=10)
        
        self.status_label = ttk.Label(btn_box, text="ì¤€ë¹„ ì™„ë£Œ", font=("Arial", 12), foreground="gray")
        self.status_label.pack(side="left", padx=20)

        # --- í•˜ë‹¨ ì˜ì—­: ëŒ€ë³¸ (ìŠ¤í¬ë¡¤ ê°€ëŠ¥, í¬ê²Œ) ---
        bottom_frame = ttk.LabelFrame(main_frame, text="ğŸ“„ ë°œí‘œ ëŒ€ë³¸ (ì‹œì„ ì´ ë‚´ë ¤ê°€ë©´ ê°ì ë©ë‹ˆë‹¤!)")
        bottom_frame.pack(side='bottom', fill='both', expand=True, pady=(10, 0))
        
        # ëŒ€ë³¸ í…ìŠ¤íŠ¸ ìœ„ì ¯ + ìŠ¤í¬ë¡¤ë°”
        self.script_text = tk.Text(bottom_frame, height=8, font=("Arial", 14), bg="white", fg="black", wrap="word")
        scrollbar = ttk.Scrollbar(bottom_frame, orient="vertical", command=self.script_text.yview)
        self.script_text.configure(yscrollcommand=scrollbar.set)
        
        scrollbar.pack(side="right", fill="y")
        self.script_text.pack(side="left", fill="both", expand=True)
        
        self.start_camera()

    # =========================================================================
    # [ê¸°ëŠ¥ ë³µêµ¬] ê¸´ì¥ ëª¨ë“œ í† ê¸€ (í…ìŠ¤íŠ¸ í•˜ì–—ê²Œ ë³€í•´ì„œ ì•ˆ ë³´ì´ëŠ” ê¸°ëŠ¥)
    # =========================================================================
    def toggle_anxiety(self):
        self.is_anxious = not self.is_anxious
        if self.is_anxious:
            self.btn_panic.config(text="ğŸ˜° ê¸´ì¥ ëª¨ë“œ: ON", bg="#ffcccc", fg="red")
            self.script_text.config(fg="white", bg="white") # ê¸€ì”¨ë¥¼ í°ìƒ‰ìœ¼ë¡œ ë³€ê²½ (ì•ˆ ë³´ì´ê²Œ)
            
            # [ê¸´ì¥ íš¨ê³¼] ì²­ì¤‘ë“¤ì´ ì¦‰ì‹œ ì‚°ë§Œí•´ì§ (Distracted)
            self.update_audience_images('distracted', 'distracted')
            
            threading.Thread(target=self.anxiety_sound_loop, daemon=True).start()
        else:
            self.btn_panic.config(text="ğŸ˜° ê¸´ì¥ ëª¨ë“œ: OFF", bg="#dddddd", fg="black")
            self.script_text.config(fg="black", bg="white") # ê¸€ì”¨ ë³µêµ¬
            
            # [ë³µêµ¬] ë‹¤ì‹œ í‰ë²”í•œ ìƒíƒœë¡œ
            self.update_audience_images('default', 'default')
    # =========================================================================
    # ë¦¬ì–¼ ì‹¬ì¥ ì‚¬ìš´ë“œ ìƒì„±ê¸°
    # =========================================================================
    
    def anxiety_sound_loop(self):
        RATE = 16000
        BPM = 115 
        DURATION = 60 / BPM 
        t = np.linspace(0, DURATION, int(RATE * DURATION), False)
        
        s1_freq = 40
        s1_envelope = np.exp(-t * 25)
        s1 = (np.sin(2 * np.pi * s1_freq * t) + 0.6 * np.sin(2 * np.pi * 25 * t)) * s1_envelope
        
        s2_delay = 0.2
        t_s2 = t - s2_delay
        s2_freq = 60
        s2_envelope = np.exp(-t_s2 * 35) * (t_s2 > 0) 
        s2 = np.sin(2 * np.pi * s2_freq * t_s2) * s2_envelope * 0.8 

        heartbeat = s1 + s2 # Heartbeat sound
        tinnitus = np.sin(2 * np.pi * 8500 * t) * 0.04 # ê³ ì£¼íŒŒìŒ
        noise = np.random.uniform(-0.015, 0.015, len(t)) # ë°±ìƒ‰ ì†ŒìŒ
        
        audio_signal = heartbeat * 1.2 + tinnitus + noise
        audio_signal = np.clip(audio_signal, -1, 1) * 32767
        audio_bytes = audio_signal.astype(np.int16).tobytes()
        
        try:
            p = pyaudio.PyAudio()
            stream = p.open(format=pyaudio.paInt16, channels=1, rate=RATE, output=True)
            while self.is_anxious:
                stream.write(audio_bytes)
                time.sleep(random.uniform(0.0, 0.03))
            stream.stop_stream(); stream.close(); p.terminate()
        except Exception as e:
            print(f"ì‚¬ìš´ë“œ ì¬ìƒ ì˜¤ë¥˜: {e}")

    def start_camera(self):
        global cap
        try:
            # 1ë‹¨ê³„: exe í™˜ê²½ì—ì„œ ê°€ì¥ ì•ˆì •ì ì¸ DSHOW ëª¨ë“œ ì‹œë„
            cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)
            
            # 2ë‹¨ê³„: DSHOWê°€ ì‹¤íŒ¨í–ˆê±°ë‚˜ ì¹´ë©”ë¼ê°€ ì•ˆ ì—´ë¦¬ë©´ ì¼ë°˜ ëª¨ë“œë¡œ ì¬ì‹œë„
            if cap is None or not cap.isOpened():
                print("âš ï¸ DSHOW ëª¨ë“œ ì‹¤íŒ¨, ì¼ë°˜ ëª¨ë“œë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                if cap: cap.release()
                cap = cv2.VideoCapture(0) # ì¼ë°˜ ëª¨ë“œ
            
            # 3ë‹¨ê³„: ê·¸ë˜ë„ ì•ˆ ë˜ë©´ -1ë²ˆ ì¥ì¹˜ ì‹œë„ (ì¼ë¶€ ë…¸íŠ¸ë¶ìš©)
            if cap is None or not cap.isOpened():
                cap = cv2.VideoCapture(-1)

            # ìµœì¢… í™•ì¸
            if not cap.isOpened():
                messagebox.showerror("ì¹´ë©”ë¼ ì˜¤ë¥˜", "ì¹´ë©”ë¼ë¥¼ ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\në‹¤ë¥¸ í”„ë¡œê·¸ë¨ì´ ì¹´ë©”ë¼ë¥¼ ì“°ê³  ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
                return

            # í•´ìƒë„ ì„¤ì • (640x360)
            cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 360)
            
            # í™”ë©´ ì—…ë°ì´íŠ¸ ì‹œì‘
            self.update_video_stream()
            
        except Exception as e:
            # ì–´ë–¤ ì˜¤ë¥˜ì¸ì§€ ì •í™•íˆ ë©”ì‹œì§€ë¡œ ë„ì›Œì¤ë‹ˆë‹¤.
            messagebox.showerror("ì¹´ë©”ë¼ ì˜¤ë¥˜", f"ì´ˆê¸°í™” ì‹¤íŒ¨ ì›ì¸:\n{e}")

    # =========================================================================
    # [í•µì‹¬ ìˆ˜ì •] ì •êµí•œ ì‹œì„  ì¶”ì  (Iris Tracking & Head Pitch)
    # =========================================================================
    def update_video_stream(self):
        global gaze_data, cap, frame_count, face_mesh 
        if not self.winfo_exists(): return
        
        try:
            if cap is None or not cap.isOpened(): return 

            ret, frame = cap.read()
            if not ret: return
            
            frame = cv2.flip(frame, 1)
            frame_count += 1
            h, w, _ = frame.shape

            # --- ê¸´ì¥ ì‹œê° íš¨ê³¼(ìŠ¤í¬ë¦° íŒí”„ íš¨ê³¼) ---       
            if self.is_anxious:
                try:
                    self.heart_phase += 0.35
                    pulse = (np.sin(self.heart_phase) + 1) / 2 
                    
                    overlay = frame.copy()
                    h, w, channels = frame.shape
                    if channels == 4: overlay[:] = (0, 0, 255, 255)   
                    else: overlay[:] = (0, 0, 255)      
                    
                    alpha = pulse * 0.25 
                    frame = cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)
                    
                    dx = random.randint(-5, 5)
                    dy = random.randint(-5, 5)
                    M = np.float32([[1, 0, dx], [0, 1, dy]])
                    frame = cv2.warpAffine(frame, M, (w, h))
                except: pass 

            # --- MediaPipe ì–¼êµ´/ì‹œì„  ë¶„ì„ ---
            script_gaze_detected = False
            
            # ì„±ëŠ¥ì„ ìœ„í•´ 2í”„ë ˆì„ë§ˆë‹¤ ë¶„ì„í•˜ì§€ë§Œ, ë…¹í™” ì¤‘ì—ëŠ” ë§¤ í”„ë ˆì„ ì²´í¬ê°€ ë” ì •í™•í•  ìˆ˜ ìˆìŒ
            # ì—¬ê¸°ì„œëŠ” 2í”„ë ˆì„ ê°„ê²© ìœ ì§€
            if frame_count % 2 == 0: 
                try:
                    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    results = face_mesh.process(rgb_frame)
                    
                    if results.multi_face_landmarks:
                        landmarks = results.multi_face_landmarks[0].landmark
                        
                        # 3D ì¢Œí‘œ ë³€í™˜
                        mesh_points = np.array([np.multiply([p.x, p.y], [w, h]).astype(int) for p in landmarks])
                        
                        # [ì•Œê³ ë¦¬ì¦˜ ë³µêµ¬] ëˆˆë™ì ìˆ˜ì§ ìœ„ì¹˜ ë¹„ìœ¨ (Vertical Gaze Ratio)
                        # ì™¼ìª½ ëˆˆ: 159(ìœ„), 145(ì•„ë˜), 468(ëˆˆë™ì)
                        # ì˜¤ë¥¸ìª½ ëˆˆ: 386(ìœ„), 374(ì•„ë˜), 473(ëˆˆë™ì)
                        
                        def get_gaze_ratio(top, bottom, iris):
                            eye_height = np.linalg.norm(top - bottom)
                            dist_to_top = np.linalg.norm(top - iris)
                            # ëˆˆì„ ê°ì•˜ê±°ë‚˜ ì¸ì‹ì´ ë¶ˆì•ˆì •í•˜ë©´ 0.5(ì •ë©´) ë°˜í™˜
                            if eye_height < 3: return 0.5 
                            return dist_to_top / eye_height

                        left_ratio = get_gaze_ratio(mesh_points[159], mesh_points[145], mesh_points[468])
                        right_ratio = get_gaze_ratio(mesh_points[386], mesh_points[374], mesh_points[473])
                        avg_ratio = (left_ratio + right_ratio) / 2
                        
                        # [í•µì‹¬ ìˆ˜ì •] ì„ê³„ê°’ ì¬ì¡°ì • (0.68)
                        # 0.50: ì •ë©´
                        # 0.57: ë„ˆë¬´ ì˜ˆë¯¼í•¨ (ê°€ë§Œíˆ ìˆì–´ë„ ê±¸ë¦¼)
                        # 0.75: ë„ˆë¬´ ë‘”ê°í•¨ (ëŒ€ë³¸ ë´ë„ ì•ˆ ê±¸ë¦¼)
                        # --> 0.68ë¡œ ì„¤ì •í•˜ì—¬ ì•ˆì •ì„± í™•ë³´
                        if avg_ratio > 0.57: 

                            script_gaze_detected = True
                            # ì‹œê°ì  í”¼ë“œë°±
                            cv2.putText(frame, "LOOKING DOWN!", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                            cv2.circle(frame, tuple(mesh_points[468]), 3, (0, 0, 255), -1)
                            cv2.circle(frame, tuple(mesh_points[473]), 3, (0, 0, 255), -1)
                        else:
                            # ì •ë©´ ì‘ì‹œ
                            cv2.circle(frame, tuple(mesh_points[468]), 3, (0, 255, 0), -1)
                            cv2.circle(frame, tuple(mesh_points[473]), 3, (0, 255, 0), -1)
                        
                        # ëˆˆ ìœ¤ê³½ì„ 
                        cv2.polylines(frame, [mesh_points[[33, 133]]], True, (200, 200, 200), 1)
                        cv2.polylines(frame, [mesh_points[[362, 263]]], True, (200, 200, 200), 1)


                    # ë°ì´í„° ì§‘ê³„
                    if is_recording:
                        gaze_data['total_frames'] += 1
                        if script_gaze_detected:
                            gaze_data['script_frames'] += 1 # ê°ì  ìš”ì¸
                        elif results.multi_face_landmarks:
                            gaze_data['looking_frames'] += 1 # ë“ì  ìš”ì¸ (ì •ë©´ ì‘ì‹œ)
                            
                except Exception as e: 
                    # print(f"Medipipe ì˜¤ë¥˜: {e}") 
                    pass

            if is_recording and out: 
                out.write(frame)
                cv2.circle(frame, (30, 30), 10, (0, 0, 255), -1)

            # í™”ë©´ í‘œì‹œë¥¼ ìœ„í•´ í¬ê¸° ì¡°ì • (640x360)
            img = ImageTk.PhotoImage(Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)).resize((640, 360))) 
            self.video_panel.configure(image=img); self.video_panel.image = img
            
            if self.winfo_exists():
                self.after(30, self.update_video_stream)
                
        except Exception as e:
            if self.winfo_exists():
                self.after(1000, self.update_video_stream)

    # =========================================================================
    # [ìˆ˜ì •ë¨] ì²­ì¤‘ ì´ë¯¸ì§€ ì—…ë°ì´íŠ¸ (í¬ê¸° 640x360ì— ë§ì¶° ì¡°ì •)
    # =========================================================================
    def update_audience_images(self, s1, s2):
        def get_image(idx, state):
            filename = f"audience{idx}_{state}.png" 
            path = resource_path(filename)
            if not os.path.exists(path): path = resource_path(f"audience{idx}_default.png")
            try:
                # í™”ë©´ ë¶„í•  í¬ê¸°ì— ë§ì¶° ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì§• (ì•½ 320x240 ì •ë„ê°€ ì ë‹¹)
                return ImageTk.PhotoImage(Image.open(path).resize((300, 225)))
            except: return None

        img1 = get_image(1, s1)
        img2 = get_image(2, s2)
        
        if img1: self.aud_labels[0].configure(image=img1); self.aud_labels[0].image = img1
        if img2: self.aud_labels[1].configure(image=img2); self.aud_labels[1].image = img2

    # =========================================================================
    # ì²­ì¤‘ í–‰ë™ ë£¨í”„
    # =========================================================================
    def audience_loop(self):
        if not is_recording: return
        if self.is_anxious:
            s1, s2 = 'distracted', 'distracted'
        else:
            states = ['default']*6 + ['focused']*2 + ['distracted']*1 + ['question']*1
            s1 = random.choice(states)
            s2 = random.choice(states)
        self.update_audience_images(s1, s2)
        if self.winfo_exists(): self.after(4000, self.audience_loop)

    # =========================================================================
    # ëŒë°œ ì§ˆë¬¸ íŠ¸ë¦¬ê±°
    # =========================================================================
    def trigger_question_event(self):
        if not self.winfo_exists(): return
        
        # ì§ˆë¬¸ì ì„ ì • (í•œ ëª…ì€ ì§ˆë¬¸, í•œ ëª…ì€ ì³ë‹¤ë´„)
        asker_idx = random.randint(0, 1)
        if asker_idx == 0: self.update_audience_images('question', 'focused')
        else: self.update_audience_images('focused', 'question')
        
        self.update()
        threading.Thread(target=self._trigger_question_thread, args=(self.script_text.get("1.0", tk.END).strip(), self.user_settings.get('atmosphere', 'ì •ë³´')), daemon=True).start()

    def _trigger_question_thread(self, script, mode):
        ai_question = None
        possible_questions = []
        if 'app_config' in globals() and hasattr(app_config, 'BACKUP_QUESTIONS'):
            possible_questions.extend(app_config.BACKUP_QUESTIONS)
        else:
            possible_questions.append("ê°€ì¥ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°í•˜ëŠ” ì ì€ ë¬´ì—‡ì¸ê°€ìš”?")

        if self.AI_AVAILABLE:
            try:
                if 'ì •ë³´' in mode: ai_question = self.imrad_validator.generate_imrad_question(script)
                elif 'ì„¤ë“' in mode: ai_question = self.dynamic_generator.generate_question(script, 'B')
                elif 'ê³µê°' in mode: ai_question = self.dynamic_generator.generate_question(script, 'C')
                if ai_question: possible_questions.append(ai_question)
            except: pass

        final_question = random.choice(possible_questions)
        if self.winfo_exists(): self.after(0, self._show_question_popup, final_question)

    def _show_question_popup(self, final_question):
        if not self.winfo_exists(): return
        self.add_marker(time.time() - start_time, 'â“')
        messagebox.showinfo("ğŸ’¡ ëŒë°œ ì§ˆë¬¸", final_question)
    
    def start_recording(self):
        global is_recording, start_time, out, speech_data, timeline_markers, gaze_data, audio_data
        if len(self.script_text.get("1.0", tk.END).strip()) < 10:
            messagebox.showwarning("ê²½ê³ ", "ëŒ€ë³¸ì„ 10ì ì´ìƒ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        if not vosk_model:
            if not messagebox.askyesno("ê²½ê³ ", "ìŒì„± ì¸ì‹ ëª¨ë¸(Vosk)ì´ ì—†ìŠµë‹ˆë‹¤. ì†Œë¦¬ ì—†ì´ ë…¹í™”ë§Œ í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
                return

        is_recording = True; start_time = time.time()
        speech_data = {"full_transcript": "", "word_count": 0, "filler_count": 0}
        # ë°ì´í„° ì´ˆê¸°í™” (script_frames í¬í•¨)
        gaze_data = {"total_frames": 0, "looking_frames": 0, "script_frames": 0}
        audio_data = {"volumes": [], "tremble_count": 0}
        timeline_markers = []
        self.raw_audio_frames = [] 
        
        try:
            fourcc = cv2.VideoWriter_fourcc(*'XVID')
            out = cv2.VideoWriter('output.avi', fourcc, 20.0, (640, 360)) # í•´ìƒë„ ë§ì¶¤
        except Exception as e:
            messagebox.showerror("ì˜¤ë¥˜", f"ë¹„ë””ì˜¤ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
            is_recording = False
            return
            
        threading.Thread(target=self.speech_recognition_thread, daemon=True).start()
        
        self.btn_start['state'] = 'disabled'; self.btn_stop['state'] = 'normal'; self.btn_question['state'] = 'normal'
        self.script_text['state'] = 'normal' # ë…¹í™” ì¤‘ì—ë„ ìŠ¤í¬ë¡¤ í•´ì•¼ í•˜ë¯€ë¡œ normal
        self.status_label.config(text="ğŸ”´ ë…¹í™” ì¤‘", foreground="red")
        self.audience_loop()

    # [ìˆ˜ì •ë¨] Vosk ê¸°ë°˜ ì‹¤ì‹œê°„ SPM(ìŒì ˆ) ì¸¡ì • ìŠ¤ë ˆë“œ
    def speech_recognition_thread(self):
        global speech_data, audio_data, pa, vosk_model
        
        SENSITIVITY = 5.0  
        RATE = 16000
        CHUNK = 4096
        
        if not vosk_model: return

        rec = KaldiRecognizer(vosk_model, RATE)
        
        try:
            stream = pa.open(format=pyaudio.paInt16, channels=1, rate=RATE, input=True, frames_per_buffer=CHUNK)
        except Exception as e:
            print(f"ë§ˆì´í¬ ì˜¤ë¥˜: {e}")
            return

        last_speech_end = time.time()
        last_vol = 0

        print(f"ğŸ¤ ë§ˆì´í¬ ë¯¼ê°ë„ {SENSITIVITY}ë°° / SPM ëª¨ë“œë¡œ ì‹œì‘")

        while is_recording:
            try:
                if stream.get_read_available() < CHUNK:
                    time.sleep(0.01)
                    continue
                
                data = stream.read(CHUNK, exception_on_overflow=False)
                
                # --- [ë¯¼ê°ë„ ì¡°ì ˆ] ---
                audio_array = np.frombuffer(data, dtype=np.int16)
                audio_array = audio_array * SENSITIVITY
                audio_array = np.clip(audio_array, -32768, 32767)
                data = audio_array.astype(np.int16).tobytes()
                # ---------------------

                self.raw_audio_frames.append(data)

                # ë³¼ë¥¨/ë–¨ë¦¼ ë¶„ì„
                rms = audioop.rms(data, 2)
                if abs(rms - last_vol) > 2000 and rms > 500: 
                    audio_data['tremble_count'] += 1
                last_vol = rms
                audio_data['volumes'].append(rms)

                #vosk ìŒì„± ì¸ì‹
                if rec.AcceptWaveform(data):
                    result = json.loads(rec.Result())
                    text = result.get('text', '')
                    
                    if text:
                        print(f"ğŸ¤ ì¸ì‹ë¨: {text}") # ë””ë²„ê¹…ìš©
                        timestamp = time.time() - start_time
                        
                        # SPM(Syllables Per Minute) ë¡œì§
                         # ê³µë°± ì œê±° í›„ ìˆœìˆ˜ ê¸€ì ìˆ˜(ìŒì ˆ)ë§Œ ì…‰ë‹ˆë‹¤.
                        syllable_count = len(text.replace(" ", "")) 
                        speech_data['full_transcript'] += text + " "

                        # ë³€ìˆ˜ ì´ë¦„ì€ word_countì§€ë§Œ, ì‹¤ì œë¡œëŠ” ì´ì œ 'ìŒì ˆ ìˆ˜'ê°€ ì €ì¥ë©ë‹ˆë‹¤.
                        speech_data['word_count'] += syllable_count 
                        
                        # ìˆœê°„ ì†ë„(Instant SPM) ê³„ì‚°
                        segment_duration = time.time() - last_speech_end
                        if segment_duration > 0.5:
                            # (ê¸€ììˆ˜ / ì‹œê°„ì´ˆ) * 60 = ë¶„ë‹¹ ê¸€ììˆ˜
                            instant_spm = (syllable_count / segment_duration) * 60
                            
                             # âš¡ SPM ê¸°ì¤€ ë§ˆì»¤ ì°ê¸° (í•œêµ­ì–´ ê¸°ì¤€)
                            # 450íƒ€ ì´ìƒ = ë§ì´ ë„ˆë¬´ ë¹ ë¦„
                            if instant_spm > 450: self.add_marker(timestamp, 'âš¡ï¸') 
                            # 200íƒ€ ì´í•˜ì´ê³  ê¸€ìê°€ ì¢€ ê¸¸ë©´ = ë§ì´ ë„ˆë¬´ ëŠë¦¼
                            elif instant_spm < 200 and syllable_count > 5: self.add_marker(timestamp, 'ğŸ¢') 
                            
                        last_speech_end = time.time()

                        if 'app_config' in globals() and hasattr(app_config, 'FILLER_WORDS'):
                             words = text.split() 
                             chunk_filler = sum(1 for w in words if w in app_config.FILLER_WORDS)
                             speech_data['filler_count'] += chunk_filler
                             if chunk_filler > 0: self.add_marker(timestamp, 'ğŸ’¬')

            except Exception as e:
                print(f"ì˜¤ë””ì˜¤ ìŠ¤ë ˆë“œ ì˜¤ë¥˜: {e}")
                continue

        stream.stop_stream()
        stream.close()
        
        # ë§ˆì§€ë§‰ ë²„í¼ ì²˜ë¦¬ (FinalResult)
        final_res = json.loads(rec.FinalResult())
        final_text = final_res.get('text', '')
        if final_text:
            speech_data['full_transcript'] += final_text + " "
            # ì—¬ê¸°ë„ ìŒì ˆ ìˆ˜ë¡œ ì €ì¥
            speech_data['word_count'] += len(final_text.replace(" ", ""))

    def add_marker(self, t, emoji):
        if not timeline_markers or (t - timeline_markers[-1]['time'] > 1.5) or timeline_markers[-1]['label'] != emoji:
            timeline_markers.append({'time': max(0.1, t), 'label': emoji})

    def stop_recording(self):
        global is_recording
        is_recording = False
        self.original_script = self.script_text.get("1.0", tk.END).strip()
        self.btn_stop['state'] = 'disabled'
        self.btn_question['state'] = 'disabled'
        self.status_label.config(text="â³ ì €ì¥ ë° ë¶„ì„ ì¤‘ (Whisper êµ¬ë™)...", foreground="blue")
        self.update()
        threading.Thread(target=self._finalize_and_analyze_thread, daemon=True).start()

    def _finalize_and_analyze_thread(self):
        global cap, out, speech_data 
        
        try:
            self.extracted_keywords = self.analysis_manager.extract_keywords_from_script(
                self.original_script, self.AI_AVAILABLE, self.text_model 
            )
        except: self.extracted_keywords = []
        
        try:
            if self.raw_audio_frames:
                wf = wave.open("output.wav", 'wb')
                wf.setnchannels(1) 
                wf.setsampwidth(2) 
                wf.setframerate(16000) 
                wf.writeframes(b''.join(self.raw_audio_frames))
                wf.close()
                print("âœ… output.wav ì €ì¥ ì™„ë£Œ.")
            else:
                print("âŒ ì €ì¥í•  ì˜¤ë””ì˜¤ ë°ì´í„° ì—†ìŒ")
                return 
        except Exception as e:
            print(f"wav ì €ì¥ ì‹¤íŒ¨: {e}")

        # Whisper í•˜ì´ë¸Œë¦¬ë“œ ë¡œì§
        # Voskê°€ ëŒ€ì¶© ë°›ì•„ì ì€ê±¸ Whisperê°€ 'ì •ë°€ ì²­ì·¨'í•˜ì—¬ ë®ì–´ì”ë‹ˆë‹¤.
        try:
            print("â³ Whisper ì •ë°€ ë¶„ì„ ì‹œì‘ (ì ì‹œë§Œ ê¸°ë‹¤ë¦¬ì„¸ìš”)...")

            # ëª¨ë¸ ë¡œë“œ (tiny, base, small ì¤‘ ì„ íƒ. smallì´ í•œêµ­ì–´ ì„±ëŠ¥/ì†ë„ ë°¸ëŸ°ìŠ¤ êµ¿)
            # device="cpu", compute_type="int8" -> CPUì—ì„œ ë¹ ë¥´ê²Œ ëŒë¦¬ê¸° ìœ„í•œ ì„¤ì •
            model = WhisperModel("small", device="cpu", compute_type="int8")

            # ë³€í™˜ ì‹¤í–‰ (beam_size=5ëŠ” ì •í™•ë„ë¥¼ ë†’ì„)
            segments, info = model.transcribe("output.wav", beam_size=5, language="ko")
            
            whisper_text = ""
            for segment in segments:
                whisper_text += segment.text + " "
            
            print(f"âœ… Whisper ë³€í™˜ ê²°ê³¼: {whisper_text}")
            # [í•µì‹¬] Voskê°€ ì‘ì„±í•œ ì—‰ì„±í•œ ëŒ€ë³¸ì„ Whisperì˜ ì™„ë²½í•œ ëŒ€ë³¸ìœ¼ë¡œ êµì²´!
            speech_data['full_transcript'] = whisper_text.strip()
            
        except Exception as e:
            print(f"âŒ Whisper ë¶„ì„ ì‹¤íŒ¨ (Vosk ê²°ê³¼ ìœ ì§€): {e}")
     
        time.sleep(1.0)
        if out: out.release(); out = None
        if cap: cap.release(); cap = None
            
        if self.winfo_exists(): 
            self.after(0, self.show_analysis_page)

    # =========================================================================
    # [ìˆ˜ì •ë¨] ë¶„ì„ í˜ì´ì§€: ê°ì  ë¡œì§ ë°˜ì˜
    # =========================================================================
    def show_analysis_page(self):
        self.clear_window()
        main_canvas = tk.Canvas(self)
        scrollbar = ttk.Scrollbar(self, orient="vertical", command=main_canvas.yview)
        scrollable_frame = ttk.Frame(main_canvas)
        scrollable_frame.bind("<Configure>", lambda e: main_canvas.configure(scrollregion=main_canvas.bbox("all")))
        canvas_frame = main_canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        main_canvas.bind("<Configure>", lambda e: main_canvas.itemconfig(canvas_frame, width=e.width))
        main_canvas.configure(yscrollcommand=scrollbar.set)
        main_canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")
        self.bind_all("<MouseWheel>", lambda e: main_canvas.yview_scroll(int(-1*(e.delta/120)), "units"))
        content = ttk.Frame(scrollable_frame, padding=30)
        content.pack(fill='both', expand=True)

        global speech_data, gaze_data, audio_data, start_time
        
        # ì‹¤ì œ ì˜¤ë””ì˜¤ ê¸¸ì´ ê¸°ë°˜ ì‹œê°„ ì¸¡ì •
        duration_min = max(0.1, (time.time() - start_time) / 60)
        try:
            # output.wav íŒŒì¼ì˜ í—¤ë”ë¥¼ ì½ì–´ì„œ ì •í™•í•œ ë…¹ìŒ ì‹œê°„(ì´ˆ)ì„ êµ¬í•¨
            with contextlib.closing(wave.open("output.wav", 'r')) as f:
                frames = f.getnframes()
                rate = f.getframerate()
                duration_sec = frames / float(rate)
                duration_min = max(0.01, duration_sec / 60)
                print(f"â±ï¸ ì‹¤ì œ ë…¹ìŒ ì‹œê°„: {duration_sec:.2f}ì´ˆ") # ë””ë²„ê¹…ìš©
        except Exception as e:
            print(f"ì‹œê°„ ê³„ì‚° ì˜¤ë¥˜(ë°±ì—… ë¡œì§ ì‚¬ìš©): {e}")
            duration_min = max(0.1, (time.time() - start_time) / 60)

        # Whisper í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        current_transcript = speech_data['full_transcript']
        
        # ê³µë°± ì œì™¸ ìˆœìˆ˜ ê¸€ì ìˆ˜ (ìŒì ˆ)
        char_count = len(current_transcript.replace(" ", ""))   

        # ì†ë„ ì ìˆ˜
        spm = int(speech_data['word_count'] / duration_min) if speech_data['word_count'] > 0 else 0 
        score_speed = max(0, 100 - int(abs(350 - spm) * 0.4))
        speed_eval = "ì ì •"
        if spm < 280: speed_eval = "ëŠë¦¼ ğŸ¢"
        elif spm > 420: speed_eval = "ë¹ ë¦„ âš¡"
        
        # ì‹œì„  ì²˜ë¦¬ ì ìˆ˜ (ê°ì  ë¡œì§ ì ìš©)
        total_frames = max(1, gaze_data['total_frames'])
        
        # 1. ì •ë©´ ì‘ì‹œìœ¨ (ê¸°ë³¸ ì ìˆ˜)
        base_gaze_score = (gaze_data['looking_frames'] / total_frames) * 100
        
        # 2. ëŒ€ë³¸ ì‘ì‹œ(Looking Down) ê°ì 
        script_penalty = (gaze_data['script_frames'] / total_frames) * 150 # ê°ì  ê°€ì¤‘ì¹˜
        
        # 3. ìµœì¢… ì‹œì„  ì ìˆ˜
        final_gaze_score = max(0, min(100, int(base_gaze_score - script_penalty)))
        
        # ì „ë‹¬ë¥  ì ìˆ˜(Whisper ê¸°ë°˜)
        import difflib
        script = self.original_script
        if len(current_transcript.strip()) > 5:
            def clean_text(text):
                return re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', text)
            clean_script = clean_text(script)
            clean_trans = clean_text(current_transcript)
            matcher = difflib.SequenceMatcher(None, clean_script, clean_trans)
            raw_score = matcher.ratio() * 100
            match_rate = int(raw_score * 1.05) 
            if match_rate > 100: match_rate = 100
            match_label_text = "ì „ë‹¬ë¥ "
        else:
            match_rate = 0
            match_label_text = "ë°ì´í„° ë¶€ì¡±"

        # ìœ ì°½ì„± ì ìˆ˜
        filler_deduction = speech_data['filler_count'] * 3
        tremble_score = max(0, 100 - int(audio_data['tremble_count'] / duration_min * 2))
        score_fluency = int((max(0, 100 - filler_deduction) + tremble_score) / 2)
        
        # ì¢…í•© ì ìˆ˜
        mode = self.user_settings.get('atmosphere', 'ì •ë³´')
        if 'ì •ë³´' in mode: total_score = int(match_rate * 0.4 + score_fluency * 0.3 + final_gaze_score * 0.2 + score_speed * 0.1)
        elif 'ì„¤ë“' in mode: total_score = int(final_gaze_score * 0.4 + score_speed * 0.2 + score_fluency * 0.2 + match_rate * 0.2)
        else: total_score = int(match_rate * 0.3 + final_gaze_score * 0.3 + score_fluency * 0.2 + score_speed * 0.2)
        self.save_history(total_score)
        
        # UI í‘œì‹œ
        tk.Label(content, text=f"ğŸ† ì¢…í•© ì ìˆ˜: {total_score}ì ", font=("Arial", 36, "bold"), fg="#007aff").pack(pady=20)
        
        if gaze_data['script_frames'] > total_frames * 0.2:
            tk.Label(content, text=f"âš ï¸ ëŒ€ë³¸ì„ ë„ˆë¬´ ìì£¼ ë³´ì…¨ìŠµë‹ˆë‹¤! (ê°ì  -{int(script_penalty)}ì )", font=("Arial", 12), fg="red").pack()

        summary = ttk.Frame(content)
        summary.pack(pady=10, fill='x')
        for i in range(4): summary.columnconfigure(i, weight=1)
        self.create_stat_card(summary, 0, f"ğŸ—£ï¸ ì†ë„ ({speed_eval})", f"{spm} SPM", score_speed)
        self.create_stat_card(summary, 1, f"ğŸ“ {match_label_text}", f"{match_rate}%", match_rate)
        self.create_stat_card(summary, 2, "ğŸ‘€ ì‹œì„  ì²˜ë¦¬", f"{final_gaze_score}ì ", final_gaze_score)
        self.create_stat_card(summary, 3, "ğŸŒŠ ìœ ì°½ì„±", f"{score_fluency}ì ", score_fluency)
        
        try:
            self.create_video_player(content)
        except Exception as e:
            tk.Label(content, text=f"ë¹„ë””ì˜¤ í”Œë ˆì´ì–´ ì˜¤ë¥˜: {e}", fg="red").pack()

            # ê·¸ë˜í”„ ê·¸ë¦¬ê¸° (ê°€ì¥ ì—ëŸ¬ ë§ì´ ë‚˜ëŠ” ê³³ - ì•ˆì „ì¥ì¹˜ ì¶”ê°€)
        try:
            self.create_score_graph(content)
        except Exception as e:
                tk.Label(content, text=f"ê·¸ë˜í”„ ìƒì„± ì‹¤íŒ¨: {e}", fg="red").pack()
        self.create_feedback_section(content, mode, match_rate, final_gaze_score, score_fluency, spm, speech_data['full_transcript'], audio_data['volumes'])
        
        ttk.Button(content, text="ì²˜ìŒìœ¼ë¡œ ëŒì•„ê°€ê¸°", command=self.show_setup_page).pack(pady=30)
        self.load_video()

        
    def create_stat_card(self, parent, col, title, value, score):
        frame = tk.Frame(parent, bg="white", bd=1, relief="solid")
        frame.grid(row=0, column=col, padx=10, sticky="nsew")
        tk.Label(frame, text=title, font=("Arial", 12, "bold"), bg="white").pack(pady=(10,5))
        tk.Label(frame, text=value, font=("Arial", 18), fg="#007aff", bg="white").pack()
        tk.Label(frame, text=f"(ì ìˆ˜: {score})", font=("Arial", 10), fg="gray", bg="white").pack(pady=(0,10))

    def create_video_player(self, parent):
        player_frame = ttk.LabelFrame(parent, text=" ğŸ¦ ë…¹í™” ì˜ìƒ ë¦¬ë·° (íƒ€ì„ë¼ì¸ í´ë¦­) ")
        player_frame.pack(fill='both', expand=True, padx=20, pady=20)
        self.vid_player_label = ttk.Label(player_frame); self.vid_player_label.pack(pady=10, fill='both', expand=True)
        self.timeline = tk.Canvas(player_frame, height=40, bg="#e9ecef"); self.timeline.pack(fill='x', padx=10)
        self.timeline.bind("<Button-1>", self.on_timeline_click)
        self.vid_slider = ttk.Scale(player_frame, from_=0, to=100, orient="horizontal", command=self.on_slider_move)
        self.vid_slider.pack(fill='x', padx=10, pady=(0, 10))
        btn_frame = ttk.Frame(player_frame); btn_frame.pack(pady=10)
        ttk.Button(btn_frame, text="â–¶ ì¬ìƒ (ì†Œë¦¬ ON)", command=self.play_video_with_sound).pack(side='left', padx=5)
        ttk.Button(btn_frame, text="â–  ì •ì§€", command=self.stop_video).pack(side='left', padx=5)

    def create_score_graph(self, parent):
        graph_frame = ttk.Frame(parent)
        graph_frame.pack(fill='x', pady=20, padx=20)

        fig, ax = plt.subplots(figsize=(8, 2.5))
        history_len = len(self.history)

        if history_len > 0:
            # Xì¶• ë°ì´í„° ìƒì„± (1, 2, 3...)
            x_ticks = range(1, history_len + 1)
            
            # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
            ax.plot(x_ticks, self.history, marker='o', linestyle='-', color='#007aff', linewidth=2)
            ax.fill_between(x_ticks, self.history, color='#007aff', alpha=0.1)
            ax.set_title("ì—°ìŠµ ì ìˆ˜ íŠ¸ë Œë“œ")
            ax.set_ylim(0, 105)
            ax.xaxis.set_major_locator(plt.MaxNLocator(integer=True))
 
            
            ax.set_xlim(0.5, history_len + 0.5)
            ax.grid(True, linestyle='--')
            
        else:
            # ë°ì´í„°ê°€ ì—†ì„ ë•Œ ë¹ˆ ê·¸ë˜í”„ ì²˜ë¦¬
            ax.set_title("ì•„ì§ ì—°ìŠµ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤")
            ax.set_yticks([])
            ax.set_xticks([])

        canvas = FigureCanvasTkAgg(fig, master=graph_frame)
        canvas.draw()
        canvas.get_tk_widget().pack(fill='both')

    def create_feedback_section(self, parent, mode_raw, match_rate, gaze_ratio, fluency, spm, transcript, volume_data):
        fb_frame = tk.LabelFrame(parent, text="ğŸ¤– AI ì½”ì¹˜ í”¼ë“œë°±", font=("Arial", 14, "bold"))
        fb_frame.pack(fill='x', pady=20, ipady=10)
        
        if 'ì •ë³´' in mode_raw: mapped_mode = 'ë…¼ë¦¬ì '; target_type_key = 'A'
        elif 'ê³µê°' in mode_raw: mapped_mode = 'ì¹œí™”ì '; target_type_key = 'C'
        else: mapped_mode = 'ì—´ì •ì '; target_type_key = 'B'
        
        final_report_text = ""
        if spm == 0 and len(transcript.strip()) < 10:
            final_report_text = "ğŸš¨ **ë°ì´í„° ë¶€ì¡±:** ìŒì„± ë°ì´í„°ê°€ ì¶©ë¶„íˆ ì¸ì‹ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        else:
            final_report_text += "--- ğŸ“ˆ AI ì½”ì¹­ ë¦¬í¬íŠ¸ (ê·œì¹™ ê¸°ë°˜) ---\n"
            style_feedback = self.analysis_manager.analyze_speech_style(transcript, mapped_mode)
            energy_feedback = self.analysis_manager.analyze_vocal_energy(volume_data, mapped_mode)
            delivery_metrics = {"spm": spm} 
            
            final_report_text += f"{style_feedback}\n{energy_feedback}\n\n"
            
            imrad_report = []
            if target_type_key == 'A': imrad_report = self.imrad_validator.validate_imrad_sections(self.original_script)
            if imrad_report: final_report_text += "--- [ë…¼ë¦¬ êµ¬ì¡° ê²½ê³ ] ---\n" + "\n".join(imrad_report) + "\n\n"
            
            final_report_text += "--- ğŸ¤– AI ì‹¬ì¸µ í”¼ë“œë°± (Gemini) ---\n"
            ai_generated_feedback = None 
            if self.AI_AVAILABLE and self.text_model: 
                try:
                    ai_generated_feedback = self.analysis_manager.generate_ai_feedback(
                        self.text_model, transcript, target_type_key, delivery_metrics, 
                        style_feedback, energy_feedback, imrad_report
                    )
                except Exception as e:
                    ai_generated_feedback = f"ì˜¤ë¥˜: {e}"
            
            if ai_generated_feedback: final_report_text += ai_generated_feedback
            else: final_report_text += "Gemini API ë¯¸ì—°ê²°ë¡œ ì‹¬ì¸µ í”¼ë“œë°±ì„ ê±´ë„ˆëœë‹ˆë‹¤."
        
        tk.Label(fb_frame, text=final_report_text, font=("Arial", 12), justify="left", wraplength=800, padx=20).pack(anchor='w', fill='x')

    def load_video(self):
        try:
            if not os.path.exists('output.avi'): return
            self.vid_cap = cv2.VideoCapture('output.avi')
            self.vid_duration = max(1, self.vid_cap.get(cv2.CAP_PROP_FRAME_COUNT) / self.vid_cap.get(cv2.CAP_PROP_FPS))
            self.is_playing = False
            self.draw_timeline()
            self.vid_cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
            self.update_frame()
        except: pass

    def draw_timeline(self):
        if not hasattr(self, 'timeline') or not self.timeline.winfo_exists(): return
        self.timeline.delete("all")
        try:
            w = self.timeline.winfo_width()
            if w < 2: w = 1100 
            self.timeline.create_line(0, 20, w, 20, fill="#ced4da", width=2)
            for m in timeline_markers:
                if self.vid_duration > 0:
                    x = (m['time'] / self.vid_duration) * w
                    self.timeline.create_text(x, 20, text=m['label'], font=("Arial", 16), tags=(str(m['time']),))
        except: pass

    def on_timeline_click(self, event):
        if not hasattr(self, 'timeline') or not self.timeline.winfo_exists(): return
        tags = self.timeline.gettags(self.timeline.find_closest(event.x, event.y))
        if tags: self.seek(float(tags[0]))

    def on_slider_move(self, val): 
        if hasattr(self, 'vid_duration'): self.seek((float(val) / 100) * self.vid_duration)
            
    def seek(self, sec):
        if hasattr(self, 'vid_cap') and self.vid_cap and self.vid_cap.isOpened():
            self.vid_cap.set(cv2.CAP_PROP_POS_FRAMES, int(sec * self.vid_cap.get(cv2.CAP_PROP_FPS)))
            self.update_frame()

    def audio_playback_thread(self):
        global pa
        CHUNK = 1024
        try:
            if not os.path.exists("output.wav"): return
            wf = wave.open("output.wav", 'rb')
            stream = pa.open(format=pa.get_format_from_width(wf.getsampwidth()),
                             channels=wf.getnchannels(), rate=wf.getframerate(), output=True)
            data = wf.readframes(CHUNK)
            while data and self.is_playing:
                stream.write(data)
                data = wf.readframes(CHUNK)
            stream.stop_stream(); stream.close(); wf.close()
        except Exception as e: print(f"ì˜¤ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜: {e}")
        self.is_playing = False

    def play_video_with_sound(self):
        if self.is_playing: return
        self.is_playing = True
        threading.Thread(target=self.audio_playback_thread, daemon=True).start()
        self.play_video_loop()

    def stop_video(self):
        self.is_playing = False

    def play_video_loop(self):
        if not self.winfo_exists() or not self.is_playing: return
        if self.vid_cap and self.vid_cap.isOpened():
            ret, frame = self.vid_cap.read()
            if ret:
                self.show_frame(frame)
                current_pos = self.vid_cap.get(cv2.CAP_PROP_POS_MSEC) / 1000
                if hasattr(self, 'vid_slider'): self.vid_slider.set((current_pos / self.vid_duration) * 100)
                self.after(33, self.play_video_loop) 
            else: 
                self.stop_video()
                self.vid_cap.set(cv2.CAP_PROP_POS_FRAMES, 0)

    def update_frame(self):
        if self.vid_cap and self.vid_cap.isOpened():
            ret, frame = self.vid_cap.read()
            if ret: self.show_frame(frame)

    def show_frame(self, frame):
        try:
            if not hasattr(self, 'vid_player_label') or not self.vid_player_label.winfo_exists(): return
            w = self.vid_player_label.winfo_width()
            if w > 1:
                target_h = int(w * (9 / 16)) 
                h, w_orig = frame.shape[:2]
                scale = target_h / h
                target_w = int(w_orig * scale)
                resized_frame = cv2.resize(frame, (target_w, target_h), interpolation=cv2.INTER_AREA)
                img = ImageTk.PhotoImage(Image.fromarray(cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)))
                self.vid_player_label.configure(image=img, anchor='center'); self.vid_player_label.image = img
            else:
                img = ImageTk.PhotoImage(Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)).resize((640, 360)))
                self.vid_player_label.configure(image=img, anchor='center'); self.vid_player_label.image = img
        except: pass 

    def show_rewriter_window(self):
        if not self.AI_AVAILABLE:
            messagebox.showerror("ì˜¤ë¥˜", "Gemini API í‚¤ê°€ ì—†ì–´ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        self.rewriter_win = tk.Toplevel(self)
        self.rewriter_win.title("ğŸ“¢ AI ëŒ€ë³¸ ì¬ì‘ì„±")
        self.rewriter_win.geometry("1000x700")
        main_frame = ttk.Frame(self.rewriter_win, padding=10); main_frame.pack(fill='both', expand=True)
        control_frame = ttk.Frame(main_frame); control_frame.pack(fill='x', pady=5)
        ttk.Label(control_frame, text="ìŠ¤íƒ€ì¼:").pack(side='left', padx=(0, 5))
        self.rewrite_mode = tk.StringVar(value='B')
        for text, mode in [('ì •ë³´í˜• (A)', 'A'), ('ì„¤ë“í˜• (B)', 'B'), ('ê³µê°í˜• (C)', 'C')]:
            ttk.Radiobutton(control_frame, text=text, variable=self.rewrite_mode, value=mode).pack(side='left', padx=5)
        text_frame = ttk.Frame(main_frame); text_frame.pack(fill='both', expand=True, pady=5)
        text_frame.columnconfigure(0, weight=1); text_frame.columnconfigure(1, weight=1); text_frame.rowconfigure(0, weight=1)
        self.original_text = tk.Text(text_frame, height=30, width=50, font=("Arial", 11)); self.original_text.grid(row=0, column=0)
        self.rewritten_text = tk.Text(text_frame, height=30, width=50, font=("Arial", 11), state='disabled'); self.rewritten_text.grid(row=0, column=1)
        action_frame = ttk.Frame(main_frame); action_frame.pack(fill='x', pady=10)
        self.rewrite_status_label = ttk.Label(action_frame, text="ì¤€ë¹„ ì™„ë£Œ", foreground="gray"); self.rewrite_status_label.pack(side='left', padx=10)
        self.rewrite_btn = ttk.Button(action_frame, text="ğŸš€ ë³€í™˜ ì‹¤í–‰", command=self.run_rewriter); self.rewrite_btn.pack(side='right')

    def run_rewriter(self):
        script = self.original_text.get("1.0", tk.END).strip()
        if len(script) < 20: return
        self.rewrite_status_label.config(text="AIê°€ ë³€í™˜ ì¤‘...", foreground="blue")
        threading.Thread(target=self._rewrite_thread_target, args=(script, self.rewrite_mode.get()), daemon=True).start()

    def _rewrite_thread_target(self, script, mode):
        try:
            res = self.ai_announcer.rewrite(script, mode)
            if self.winfo_exists(): self.after(0, self.update_rewriter_ui, res)
        except Exception as e:
            if self.winfo_exists(): self.after(0, self.update_rewriter_ui, f"ì˜¤ë¥˜: {e}")

    def update_rewriter_ui(self, res):
        if hasattr(self, 'rewriter_win') and self.rewriter_win.winfo_exists():
            self.rewritten_text.config(state='normal'); self.rewritten_text.delete("1.0", tk.END); self.rewritten_text.insert("1.0", res); self.rewritten_text.config(state='disabled')
            self.rewrite_status_label.config(text="ì™„ë£Œ" if "ì˜¤ë¥˜" not in res else "ì‹¤íŒ¨", foreground="green" if "ì˜¤ë¥˜" not in res else "red")

if __name__ == "__main__":
    app = App()
    app.mainloop()
